\chapter{Basic Types} \label{chap:basic-types}
This chapter provides a brief overview of the basic interfaces and usage of the
provided data types. Operations on the various types are explained in
Chapter \ref{chap:operations}. For full details, refer to the reference pages
in the folder \texttt{doc/doxygen}.

\section {Scalar Type}
The scalar type \lstinline|scalar<T>| with template parameter T
denoting the underlying CPU scalar type (\lstinline|float| and \lstinline|double|, if supported - see Tab.~\ref{tab:double-precision-GPUs}) represents a
single scalar value on the computing device. \lstinline|scalar<T>| is designed to behave much
like a scalar type on conventional host-based CPU processing, but library users have to keep in mind that
every operation on \lstinline|scalar<T>| may require the launch of an appropriate
compute kernel on the GPU, thus making the operation much slower then the conventional CPU equivalent.
Even if the host-based computing backend of {\ViennaCL} is used, some (small) overheads occur.

\NOTE{Be aware that operations between objects of type \lstinline|scalar<T>|
(e.g.~additions. comparisons) have large overhead on GPU backends. A
separate compute kernel launch is required for every operation in such case.}

\subsection{Example Usage}
The scalar type of {\ViennaCL} can be used just like the built-in
types, as the following snippet shows:
\begin{lstlisting}
  float cpu_float = 42.0f;
  double cpu_double = 13.7603;
  viennacl::scalar<float>  gpu_float(3.1415f);
  viennacl::scalar<double> gpu_double = 2.71828;

  //conversions and t
  cpu_float = gpu_float;
  gpu_float = cpu_double;  //automatic transfer and conversion

  cpu_float = gpu_float * 2.0f;
  cpu_double = gpu_float - cpu_float;
\end{lstlisting}
Mixing built-in types with the {\ViennaCL} scalar is usually not a
problem. Nevertheless, since every operation requires {\OpenCL} calls, such
arithmetics should be used sparsingly.

\NOTE{In the present version of {\ViennaCL}, it is not possible to assign a \lstinline|scalar<float>| to a \lstinline|scalar<double>| directly.}

\subsection{Members}
Apart from suitably overloaded operators that mimic the behavior of the
respective CPU counterparts, only a single public member function
\lstinline|handle()| is available, cf.~Tab.~\ref{tab:scalar-interface}.

\begin{table}[tb]
\begin{center}
\begin{tabular}{p{6.5cm}|p{8.5cm}}
Interface & Comment\\
\hline
\texttt{v.handle()}   & The memory handle (CPU, {\CUDA}, or {\OpenCL}) \\
\end{tabular}
\caption{Interface of \texttt{vector$<$T$>$} in \ViennaCL. Destructors and
operator overloads for BLAS are not listed.}
\label{tab:scalar-interface}
\end{center}
\end{table}



\section{Vector Type}
The main vector type in {\ViennaCL} is \texttt{vector$<$T, alignment$>$},
representing a chunk of memory on the compute device. \texttt{T} is the
underlying scalar type (either \texttt{float} or \texttt{double} if supported, cf.~Tab.~\ref{tab:double-precision-GPUs}, complex types
are not supported in \ViennaCLversion) and the optional argument \texttt{alignment} denotes the memory
the vector is aligned to (in multiples of \texttt{sizeof(T)}). For example, a
vector with a size of 55 entries and an alignment of 16 will reside in a
block of memory equal to 64 entries. Memory alignment is fully
transparent, so from the end-user's point of view, \texttt{alignment} allows to
tune {\ViennaCL} for maximum speed on the available compute device.

At construction, \texttt{vector$<$T, alignment$>$} is initialized to have the
supplied length, but the memory is not initialized to zero. Another difference
to CPU implementations is that accessing single vector elements is very costly,
because every time an element is
accessed, it has to be transferred from the CPU to the compute device or vice
versa.
\subsection{Example Usage}
The following code snippet shows the typical use of the vector type provided by
{\ViennaCL}. The overloaded function \texttt{copy()} function, which is used similar to
\lstinline|std::copy()| from the C++ Standard Template Library (STL), should be used for writing vector entries:
\begin{lstlisting}
std::vector<ScalarType>      stl_vec(10);
viennacl::vector<ScalarType> vcl_vec(10);

//fill the STL vector:
for (unsigned int i=0; i<vector_size; ++i)
  stl_vec[i] = i;

//copy content to GPU vector (recommended initialization)
copy(stl_vec.begin(), stl_vec.end(), vcl_vec.begin());

//manipulate GPU vector here

//copy content from GPU vector back to STL vector
copy(vcl_vec.begin(), vcl_vec.end(), stl_vec.begin());
\end{lstlisting}

\TIP{The function copy() does not assume that the values of the supplied CPU
object are located in a linear memory sequence. If this is the case, the
function \lstinline{fast_copy} provides better performance.}

Once the vectors are set up on the GPU, they can be used like objects on the CPU
(refer to Chapter \ref{chap:operations} for more details):
\begin{lstlisting}
// let vcl_vec1 and vcl_vec2 denote two vector on the GPU
vcl_vec1 *= 2.0;
vcl_vec2 += vcl_vec1;
vcl_vec1 = vcl_vec1 - 3.0 * vcl_vec2;
\end{lstlisting}

\subsection{Members}
At construction, \texttt{vector$<$T, alignment$>$} is initialized to have the
supplied length, but memory is not initialized. If initialization is desired,
the memory can be initialized with zero values using the member function
\lstinline|clear()|. See
Tab.~\ref{tab:vector-interface} for other member functions.

\NOTE{Accessing single elements of a vector using operator() or operator[] is
very slow for GPUs due to PCI-Express latency! Use with care!}

\begin{table}[tb]
\begin{center}
\begin{tabular}{p{6.5cm}|p{8.5cm}}
Interface & Comment\\
\hline
\texttt{CTOR(n)}    & Constructor with number of entries \\
\texttt{v(i)}       & Access to the $i$-th element of v (slow for GPUs!) \\
\texttt{v[i]}       & Access to the $i$-th element of v (slow for GPUs!) \\
\texttt{v.clear()}  & Initialize v with zeros \\
\texttt{v.resize(n, bool preserve)}    & Resize v to length n. Preserves old values if bool is true. \\
\texttt{v.begin()}  & Iterator to the begin of the matrix \\
\texttt{v.end()}    & Iterator to the end of the matrix \\
\texttt{v.size()}   & Length of the vector \\
\texttt{v.swap(v2)} & Swap the content of v with v2 \\
\texttt{v.internal\_size()} & Returns the number of entries allocated on the GPU (taking alignment into account) \\
\texttt{v.empty()}   & Shorthand notation for \texttt{v.size() == 0} \\
\texttt{v.clear()}   & Sets all entries in v to zero \\
\texttt{v.handle()}  & Returns the memory handle (needed for custom kernels, see Chap.~\ref{chap:custom})
\end{tabular}
\caption{Interface of \texttt{vector$<$T$>$} in \ViennaCL. Destructors and
operator overloads for BLAS are not listed.}
\label{tab:vector-interface}
\end{center}
\end{table}

One important difference to pure CPU implementations is that the bracket operator
as well as the parenthesis operator are very slow, because for each access an {\OpenCL}
data transfer has to be initiated. The overhead of this transfer is orders of
magnitude. For example:
  \begin{lstlisting}
// fill a vector on CPU
for (size_t i=0; i<cpu_vector.size(); ++i)
    cpu_vector(i) = 1e-3f;

// fill a ViennaCL vector - VERY SLOW with GPU backends!!
for (size_t i=0; i<gpu_vector.size(); ++i)
    vcl_vector(i) = 1e-3f;
\end{lstlisting}
The difference in execution speed is typically several orders of magnitude,
therefore direct vector element access should be used only if a very small
number of entries is accessed in this way. A much faster initialization is as
follows:

  \begin{lstlisting}
// fill a vector on CPU
for (long i=0; i<cpu_vector.size(); ++i)
    cpu_vector(i) = 1e-3f;

// fill a vector on GPU with data from CPU - faster versions:
copy(cpu_vector, vcl_vector);                                   //option 1
copy(cpu_vector.begin(), cpu_vector.end(), vcl_vector.begin()); //option 2
\end{lstlisting}
In this way, setup costs for the CPU vector and the {\ViennaCL} vector are comparable.

\section{Dense Matrix Type}
\texttt{matrix$<$T, F, alignment$>$} represents a dense matrix with interface listed in
Tab.~\ref{tab:matrix-interface}. The second optional template argument \texttt{F}
specifies the storage layout and defaults to \texttt{row\_major}. As an alternative, a \lstinline|column_major| memory layout can be used.
The third template argument \texttt{alignment} denotes an alignment for the rows and columns for row-major and column-major memory layout (cf.~\texttt{alignment} for the \texttt{vector} type).

\subsection{Example Usage}
The use of \texttt{matrix$<$T, F$>$} is similar to that of the counterpart in {\ublas}. The operators are overloaded similarly.

\begin{lstlisting}
 //set up a 3 by 5 matrix:
 viennacl::matrix<float>  vcl_matrix(4, 5);

 //fill it up:
 vcl_matrix(0,2) = 1.0;
 vcl_matrix(1,2) = -1.5;
 vcl_matrix(2,0) = 4.2;
 vcl_matrix(3,4) = 3.1415;
\end{lstlisting}

\NOTE{Accessing single elements of a matrix using \texttt{operator()} is very slow on GPU backends! Use with care!}

A much better way is to initialize a dense matrix using the provided \texttt{copy()} function:
\begin{lstlisting}
//copy content from CPU matrix to GPU matrix
copy(cpu_matrix, gpu_matrix);

//copy content from GPU matrix to CPU matrix
copy(gpu_matrix, cpu_matrix);
\end{lstlisting}
The type requirement on the \texttt{cpu\_matrix} is that \texttt{operator()} can be used for accessing entries, that a member function \texttt{size1()} returns the number of rows and that \texttt{size2()} returns the number of columns.
Please refer to Chap.~\ref{chap:other-libs} for an overview of other libraries for which an overload of \texttt{copy()} is provided.

\subsection{Members}

The members are listed in Tab.~\ref{tab:matrix-interface}. The usual operator overloads are not listed explicitly

\begin{table}[tb]
\begin{center}
\begin{tabular}{p{6.5cm}|p{8.5cm}}
Interface & Comment\\
\hline
\texttt{CTOR(nrows, ncols)}    & Constructor with number of rows and columns \\
\texttt{mat(i,j)}    & Access to the element in the $i$-th row and the $j$-th column of mat \\
%\texttt{mat.clear()}    & Initialize mat with zeros \\
\parbox{6cm}{\texttt{mat.resize(m, n, \\
           \hphantom{mat.resize(}bool preserve)}}    & Resize mat to m rows and n columns. Currently, the boolean flag is ignored and entries always discarded. \\
%\texttt{mat.begin()}   & Iterator to the begin of the matrix \\
%\texttt{mat.end()}   & Iterator to the end of the matrix \\
\texttt{mat.size1()}            & Number of rows in mat \\
\texttt{mat.internal\_size1()}   & Internal number of rows in mat \\
\texttt{mat.size2()}            & Number of columns in mat \\
\texttt{mat.internal\_size2()}   & Internal number of columns in mat \\
\texttt{mat.clear()}   & Sets all entries in v to zero \\
\texttt{mat.handle()}  & Returns the memory handle (needed for custom kernels, see Chap.~\ref{chap:custom})
\end{tabular}
\caption{Interface of the dense matrix type \texttt{matrix$<$T, F$>$} in
\ViennaCL. Constructors, Destructors and operator overloads for BLAS are not
listed.}
\label{tab:matrix-interface}
\end{center}
\end{table}


\section{Sparse Matrix Types}

There are two different sparse matrix types provided in {\ViennaCL}, \texttt{compressed\_matrix} and \texttt{coordinate\_matrix}.

\TIP{In {\ViennaCLversion}, the use of \texttt{compressed\_matrix} is encouraged over \texttt{coordinate\_matrix}}

\subsection{Compressed Matrix}
\texttt{compressed\_matrix$<$T, alignment$>$} represents a sparse
matrix using a compressed sparse row scheme. Again, \texttt{T} is the floating point type. \texttt{alignment} is the alignment and defaults to \texttt{1} at present.
In general, sparse matrices should be set up on the
CPU and then be pushed to the compute device using \texttt{copy()}, because dynamic memory management of sparse matrices is not provided on {\OpenCL} compute devices such as GPUs.

\begin{table}[tb]
\begin{center}
\begin{tabular}{p{6.5cm}|p{8cm}}
Interface & Comment\\
\hline
\texttt{CTOR(nrows, ncols)}    & Constructor with number of rows and columns \\
%\texttt{mat(i,j) const}    & Read-only access to the element in the $i$-th row and the $j$-th column of mat \\
%\texttt{mat.readFrom(PointerType prows, PointerType pcols, PointerType pentries)} & Fill mat with the values from the supplied piece of memory. \\
%\texttt{mat.writeTo(PointerType prows, PointerType pcols, PointerType pentries)}  & Fill the supplied piece of memory with values from mat. \\ \\
\texttt{mat.set()}    & Initialize mat with the data provided as arguments \\
\texttt{mat.reserve(num)}    & Reserve memory for up to \texttt{num} nonzero entries \\
%\texttt{mat.clear()}    & Initialize mat with zeros \\

\texttt{mat.size1()}            & Number of rows in mat \\
\texttt{mat.size2()}            & Number of columns in mat \\
\texttt{mat.nnz()}		& Number of nonzeroes in mat \\
\parbox{6cm}{\texttt{mat.resize(m, n, \\
           \hphantom{mat.resize(}bool preserve)}}    & Resize mat to m rows and n columns. Currently, the boolean flag is ignored and entries always discarded. \\
\texttt{mat.handle1()}  & Returns the memory handle holding the row indices (needed for custom kernels, see Chap.~\ref{chap:custom}) \\
\texttt{mat.handle2()}  & Returns the memory handle holding the column indices  (needed for custom kernels, see Chap.~\ref{chap:custom}) \\
\texttt{mat.handle()}  & Returns the memory handle holding the entries (needed for custom kernels, see Chap.~\ref{chap:custom})
\end{tabular}
\caption{Interface of the sparse matrix type \texttt{compressed\_matrix$<$T, F$>$} in \ViennaCL. Destructors and operator overloads for BLAS are not listed.}
\label{tab:compressed-matrix-interface}
\end{center}
\end{table}

\subsubsection{Example Usage} \label{sec:compressed-matrix-example}
The use of \texttt{compressed\_matrix$<$T, alignment$>$} is similar to that of the counterpart in {\ublas}. The operators are overloaded similarly.
There is a direct interfacing with the standard implementation using a vector of maps from the STL:
\begin{lstlisting}
 //set up a sparse 3 by 5 matrix on the CPU:
 std::vector< std::map< unsigned int, float> > cpu_sparse_matrix(4);

 //fill it up:
 cpu_sparse_matrix[0][2] = 1.0;
 cpu_sparse_matrix[1][2] = -1.5;
 cpu_sparse_matrix[3][0] = 4.2;

 //set up a sparse ViennaCL matrix:
 viennacl::compressed_matrix<float>  vcl_sparse_matrix(4, 5);

 //copy to OpenCL device:
 copy(cpu_sparse_matrix, vcl_sparse_matrix);

 //copy back to CPU:
 copy(vcl_sparse_matrix, cpu_sparse_matrix);
\end{lstlisting}
The \texttt{copy()} functions can also be used with a generic sparse matrix data type fulfilling the following requirements:
\begin{itemize}
 \item The \texttt{const\_iterator1} type is provided for iteration along increasing row index
 \item The \texttt{const\_iterator2} type is provided for iteration along increasing column index
 \item \texttt{.begin1()} returns an iterator pointing to the element with indices $(0,0)$.
 \item \texttt{.end1()} returns an iterator pointing to the end of the first column
 \item When copying to the cpu type: Write operation via \texttt{operator()}
 \item When copying to the cpu type: \texttt{resize(m,n,preserve)} member (cf.~Tab.~\ref{tab:compressed-matrix-interface})
\end{itemize}
The iterator returned from the cpu sparse matrix type via \texttt{begin1()} has to fulfill the following requirements:
\begin{itemize}
 \item \texttt{.begin()} returns an column iterator pointing to the first nonzero element in the particular row.
 \item \texttt{.end()} returns an iterator pointing to the end of the row
 \item Increment and dereference
\end{itemize}
For the sparse matrix types in {\ublas}, these requirements are all fulfilled. Please refer to Chap.~\ref{chap:other-libs} for an overview
of other libraries for which an overload of \texttt{copy()} is provided.

\subsubsection{Members}
The interface is described in Tab.~\ref{tab:compressed-matrix-interface}.

\subsection{Coordinate Matrix}
In the second sparse matrix type, \texttt{coordinate\_matrix$<$T, alignment$>$},
entries are stored as triplets \texttt{(i,j,val)}, where \texttt{i} is the row index, \texttt{j} is the column index and \texttt{val} is the entry.
Again, \texttt{T} is the floating point type. The optional \texttt{alignment} defaults to \texttt{1} at present.
In general, sparse matrices should be set up on the
CPU and then be pushed to the compute device using \texttt{copy()}, because dynamic memory management of sparse matrices is not provided on {\OpenCL} compute devices such as GPUs.

\begin{table}[tb]
\begin{center}
\begin{tabular}{p{6.5cm}|p{8cm}}
Interface & Comment\\
\hline
\texttt{CTOR(nrows, ncols)}    & Constructor with number of rows and columns \\
%\texttt{mat(i,j) const}    & Read-only access to the element in the $i$-th row and the $j$-th column of mat \\
%\texttt{mat.readFrom(PointerType prows, PointerType pcols, PointerType pentries)} & Fill mat with the values from the supplied piece of memory. \\
%\texttt{mat.writeTo(PointerType prows, PointerType pcols, PointerType pentries)}  & Fill the supplied piece of memory with values from mat. \\ \\
\texttt{mat.reserve(num)}    & Reserve memory for \texttt{num} nonzero entries \\
\texttt{mat.size1()}            & Number of rows in mat \\
\texttt{mat.size2()}            & Number of columns in mat \\
\texttt{mat.nnz()}		& Number of nonzeroes in mat \\
\parbox{6cm}{\texttt{mat.resize(m, n, \\
           \hphantom{mat.resize(}bool preserve)}}    & Resize mat to m rows and n columns. Currently, the boolean flag is ignored and entries always discarded. \\
%\texttt{mat.clear()}    & Initialize mat with zeros \\
\texttt{mat.resize(m, n)}    & Resize mat to m rows and n columns. Does not preserve old values. \\
\texttt{mat.handle12()}  & Returns the memory handle holding the row and column indices (needed for custom kernels, see Chap.~\ref{chap:custom}) \\
\texttt{mat.handle()}  & Returns the memory handle holding the entries (needed for custom kernels, see Chap.~\ref{chap:custom})
\end{tabular}
\caption{Interface of the sparse matrix type \texttt{coordinate\_matrix$<$T, A$>$} in \ViennaCL. Destructors and operator overloads for BLAS are not listed.}
\label{tab:coordinate-matrix-interface}
\end{center}
\end{table}

\subsubsection{Example Usage}
The use of \texttt{coordinate\_matrix$<$T, alignment$>$} is similar to that of the first sparse matrix type
\texttt{compressed\_matrix$<$T, alignment$>$}, thus we refer to Sec.~\ref{sec:compressed-matrix-example}


\subsubsection{Members}
The interface is described in Tab.~\ref{tab:coordinate-matrix-interface}.

%\TIP{In {\ViennaCLversion} the use of \lstinline|compressed\_matrix| over \lstinline|coordinate\_matrix| is encouraged due to better performance!}
\NOTE{Note that only a few preconditioners work with \lstinline|coordinate_matrix| so far, cf.~ Sec.~\ref{sec:preconditioner}.}


\subsection{ELL Matrix}
A sparse matrix in ELL format of type \lstinline|ell_matrix| is stored in a block of memory of size $N \times n_{\max}$, where $N$ is the number of rows of the matrix and $n_{\max}$ is the maximum number of nonzeros per row.
Rows with less than $n_{\max}$ entries are padded with zeros. In a second memory block, the respective column indices are stored.

The ELL format is well suited for matrices where most rows have approximately the same number of nonzeros.
This is often the case for matrices arising from the discretization of partial differential equations using e.g.~the finite element method.
On the other hand, the ELL format introduces substantial overhead if the number of nonzeros per row varies a lot.

For an example use of an \lstinline|ell_matrix|, have a look at \lstinline|examples/benchmarks/sparse.cpp|.

\NOTE{Note that preconditioners in Sec.~\ref{sec:preconditioner} do not work with \lstinline|ell_matrix| yet.}

\subsection{Hybrid Matrix}
The higher performance of the ELL format for matrices with approximately the same number of entries per row
and the higher flexibility of the CSR format is combined in the \lstinline|hyb_matrix| type, where the main part of the system matrix is stored in ELL format and excess entries are stored in CSR format.

For an example use of an \lstinline|hyb_matrix|, have a look at \lstinline|examples/benchmarks/sparse.cpp|.

\NOTE{Note that preconditioners in Sec.~\ref{sec:preconditioner} do not work with \lstinline|hyb_matrix| yet.}

\section{Proxies}
Similar to {\ublas}, {\ViennaCL} provides \lstinline|range| and \lstinline|slice| objects in order to conveniently manipulate dense submatrices and vectors. The functionality is
provided in the headers \lstinline|viennacl/vector_proxy.hpp| and \lstinline|viennacl/matrix_proxy.hpp| respectively.
A range refers to a contiguous integer interval and is set up as
\begin{lstlisting}
 std::size_t lower_bound = 1;
 std::size_t upper_bound = 7;
 viennacl::range r(lower_bound, upper_bound);
\end{lstlisting}
A slice is similar to a range and allows in addition for arbitrary increments (\emph{stride}).
For example, to create a slice consisting of the indices $2, 5, 8, 11, 14$, the code
\begin{lstlisting}
 std::size_t start  = 2;
 std::size_t stride = 3;
 std::size_t size   = 5
 viennacl::slice s(start, stride, size);
\end{lstlisting}

In order to address a subvector of a vector \lstinline|v| and a submatrix of a matrix \lstinline|M|, the proxy objects \lstinline|v_sub| and \lstinline|M_sub|
are created as follows:
\begin{lstlisting}
 typedef viennacl::vector<ScalarType>                      VectorType;
 typedef viennacl::matrix<ScalarType, viennacl::row_major> MatrixType;

 viennacl::vector_range<VCLVectorType> v_sub(v, r);
 viennacl::matrix_range<VCLMatrixType> M_sub(M, r, r);
\end{lstlisting}
As a shortcut, one may use the free function \lstinline|project()| in order to avoid having to write the type explicitly:
\begin{lstlisting}
 project(v, r);    //returns a vector_range as above
 project(M, r, r); //returns a matrix_range as above
\end{lstlisting}
In the same way \lstinline|vector_slice|s and \lstinline|matrix_slice|s are set up.

The proxy objects can now be manipulated in the same way as vectors and dense matrices. In particular, operations such as vector proxy additions and matrix
additions work as usual, e.g.
\begin{lstlisting}
 vcl_sub += vcl_sub; //or project(v, r) += project(v, r);
 M_sub += M_sub;     //or project(M, r, r) += project(M, r, r);
\end{lstlisting}
 Submatrix-Submatrix products are computed in the same manner and are handy for many block-based linear algebra algorithms.

\TIP{Example code can be found in \lstinline|examples/tutorial/vector-range.cpp| and \lstinline|examples/tutorial/matrix-range.cpp|}
